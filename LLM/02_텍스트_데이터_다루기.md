# 02 텍스트 데이터 다루기



사전 훈련 단계에서 LLM은 텍스트를 한 번에 한 단어씩 처리



### 데이터 샘플링 파이프라인

=> 데이터 준비 & 샘플링



## 2.1 단어 임베딩 이해하기

- 벡터: 
  - (파이토치) 1차원 텐서

- 텐서: 
  - 차원 개수를 나타내는 차수로 특징이 결정되는 수학적 객체
  - (파이토치) 배열과 유사한 구조를 위한 데이터 컨테이너

- 임베딩(embedding): 데이터를 벡터 형태로 변환하는 개념



### 문장이나 단락 임베딩

RAG(retrieval-augmented generation)

- 생성과 검색을 겹합하여 관련 정보를 추출



### 단어 임베딩

Word2Vec

- 타깃 단어가 주어지면 문맥 단어를 예측
- 또는 그 반대의 방식으로 신경망을 훈련
- 차원이 높을수록 미묘한 관계를 잘 감지
- 효율성 떨어짐



## 2.2 텍스트 토큰화하기



토큰화 텍스트 파일을 단어로 분할 작업

=> 추후에는 훈련된 토크나이저 사용



## 2.3 토큰을 토큰 ID로 변환하기

토큰 ID를 임베딩 벡터로 변환하기 전의 중간 단계

토큰ID로 매핑하려면 어휘사전(vocabulary)을 먼저 구축해야함



## 2.4 특수 문맥 토큰 추가하기

- [BOS] (beginning of sequence): 텍스트의 시작을 표시
- [EOS] (end of sequence): 
  - 텍스트의 끝에 위치
  - 관련이 없는 여러 개의 텍스트를 연결할때 유용
- [PAD] (padding)
  - 모든 텍스트의 길이를 동일하게 맞추기 위해 빈자리에 넣어서 사용.



## 2.5 바이트 페어 인코딩



### 바이트 페어 인코딩(BPE)

GPT-2, GPT-3, ChatGPT에 사용된 모델과 같은 LLM 훈련에 사용



```sh
pip install tiktoken
```



체크 포인트

1. <|endoftext|> 토큰은 20256과 같이 비교적 큰 토큰 ID에 할당
2. someunkonwnPlace 같은 알지 못하는 단어를 정확하게 인코딩&디코딩
   - BPE 알고리즘은 어휘사전에 없는 단어를 더 작은 부분단어로 처리



BPE 정리해보면

1. 반복적으로 자주 등장하는 문자를 부분단어로 합치고
2. 다시 자주 등장하는 부분단어를 단어로 합쳐서 어휘사전을 구축하는 방식

EX)

1. 모든 개별 문자를 어휘사전에 추가
2.  "d", "e" => "de"



## 2.6 슬라이딩 윈도로 데이터 샘플링하기



토큰을 임베딩으로 바꾸기 전 작업

1. LLM 훈련에 필요한 입력-타깃 쌍을 생성
2. 입력 데이터셋을 순회하면서 파이토치 텐서로 입력과 타깃을 반환하는 효율적인 데이터 로더 구현



> 파이토치 Dataset 클래스 => 부록 A.6 참고



만약 스트라이드가 입력 윈도 크기와 같으면 배치 사이에 중첩되는 토큰이 없음.

배치사이에 중첩이 있으면 과대적합이 증가.



> 과대적합(overfitting)이란?
>
>  머신러닝이나 딥러닝 모델이 학습 데이터에는 매우 잘 맞지만, 새로운 데이터(테스트 데이터)에는 일반화가 잘 되지 않는 현상.
> 즉, 모델이 학습 데이터의 패턴뿐만 아니라 노이즈(잡음)까지 과하게 학습해서, 실제로는 성능이 떨어지는 상태.



## 2.7 토큰 임베딩 만들기



토큰 ID를 임베딩 벡터로 변환하는 과정

벡터를 랜덤한 값으로 초기화



> 역전파 => 부록 A.4 참고



임베딩 층은 토큰ID를 기반으로 가중치 행렬에서 행을 추출하는 검색 연산을 수행.



## 2.8 단어 위치 인코딩하기



절대 위치 임베딩

- 시퀀스의 특정 위치에 직접 연관
- 입력 시퀀스의 각 위치에 대해서 고유한 임베딩이 토큰 임베딩에 더해져 정확한 위치 정보를 추가

상대 위치 임베딩

- 상대적인 위치 또는 토큰 사이의 거리를 강조
- 모델이 정확한 위치가 아니라 멀리 떨어져 있는 정보를 바탕으로 관계를 학습
- 길이가 다른 시퀀스에도 더 잘 일반화될 수 있음















